{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-20T19:43:24.439682Z","iopub.status.busy":"2022-11-20T19:43:24.439162Z","iopub.status.idle":"2022-11-20T19:43:26.929679Z","shell.execute_reply":"2022-11-20T19:43:26.928507Z","shell.execute_reply.started":"2022-11-20T19:43:24.439591Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"]}],"source":["import jax\n","import jax.numpy as jnp\n","from jax import jit, random\n","import numpy as np\n","import optax\n","\n","BATCH_SIZE = 100\n","LEARNING_RATE = 1e-3\n","LAYER_SIZES = [1, 25, 50, 30, 1]\n","NUM_TRAIN_STEPS = 1_000\n","NUM_EPOCHS = 3_000\n","#OPTIMIZER = optax.adam(learning_rate=LEARNING_RATE)\n","#OPTIMIZER = optax.yogi(learning_rate=LEARNING_RATE)\n","#OPTIMIZER = optax.adabelief(learning_rate=LEARNING_RATE)\n","#OPTIMZER = optax.fromage\n","OPTIMIZER = optax.optimistic_gradient_descent(learning_rate=LEARNING_RATE)\n","ACTIVATION = jnp.tanh\n","#ACTIVATION = jax.nn.relu\n","#ACTIVATION = jax.nn.sigmoid\n","#ACTIVATION = jax.nn.silu\n","#ACTIVATION = jax.nn.softplus\n","#ACTIVATION = jax.nn.softmax\n","\n","t = jnp.linspace(start=0, stop=20, num=1000)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- See [Activation Functions](https://jax.readthedocs.io/en/latest/jax.nn.html?highlight=sigmoid) for a list of possible activations.\n","- See [Optimizers](https://optax.readthedocs.io/en/latest/api.html#optax.yogi) for a list of optimizers."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# A helper function to randomly initialize weights and biases\n","# for a dense neural network layer\n","def random_layer_params(m, n, key, scale=1e-2):\n","  w_key, b_key = random.split(key)\n","  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,1))\n","\n","# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n","def init_network_params(sizes, key):\n","  \"\"\"\n","  returns:\n","  <list> of length: (#layers - 1)\n","\n","  Each list item contains a <tuple> of len(): 2 (Weights and Biases)\n","\n","  Each tuple consists of 2 Arrays of dimension: \n","\n","  \"\"\"\n","  \n","  keys = random.split(key, len(sizes))\n","  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n","\n","initial_params = init_network_params(LAYER_SIZES, random.PRNGKey(0))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:43:39.140148Z","iopub.status.busy":"2022-11-20T19:43:39.139751Z","iopub.status.idle":"2022-11-20T19:43:39.302888Z","shell.execute_reply":"2022-11-20T19:43:39.301707Z","shell.execute_reply.started":"2022-11-20T19:43:39.140116Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Array(0.02171926, dtype=float32)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["def net(params, x):\n","    activations = x\n","    for w, b in params[:-1]:\n","        outputs = jnp.dot(w, activations) + b\n","        activations = ACTIVATION(outputs)\n","\n","    final_w, final_b = params[-1]\n","    output = jnp.dot(final_w, activations) + final_b\n","    return output.reshape()\n","\n","# check\n","net(initial_params, 2.0)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["Array(0.02171909, dtype=float32)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["m = 1.\n","c = 0.1\n","k = 1.\n","y0 = 1.\n","y0_prime = 0.\n","\n","@jit\n","def ode(params, t):\n","    nn = lambda t: net(params, t)\n","    dnn = jax.grad(nn)\n","    ddnn = jax.grad(dnn)\n","    return m * ddnn(t) + c * dnn(t) + k * nn(t)\n","\n","# check\n","ode(initial_params, 3.0)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["Array(0.9575043, dtype=float32)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["ode_batched = jax.vmap(ode, in_axes=[None, 0])\n","dnn = jit(jax.grad(net, argnums=1))\n","\n","@jit\n","def loss(params: optax.Params, batch: jnp.ndarray) -> jnp.ndarray:\n","    y_hat = ode_batched(params, batch)\n","    ode_loss = jnp.mean(y_hat ** 2)\n","    init_loss_1 = (net(params, 0.) - y0) ** 2\n","    init_loss_2 = (dnn(params, 0.) - y0_prime) ** 2\n","\n","    return ode_loss + init_loss_1 + init_loss_2\n","\n","# check\n","loss(initial_params, jnp.array([1., 2.]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["key = jax.random.PRNGKey(42)\n","\n","def fit(params: optax.Params, optimizer: optax.GradientTransformation, key) -> optax.Params:\n","  opt_state = optimizer.init(params)\n","\n","  @jit\n","  def step(params, opt_state, batch):\n","    loss_value, grads = jax.value_and_grad(loss)(params, batch)\n","    updates, opt_state = optimizer.update(grads, opt_state, params)\n","    params = optax.apply_updates(params, updates)\n","    return params, opt_state, loss_value\n","\n","  for epoch in range(NUM_EPOCHS):\n","      shuffle_key, key = jax.random.split(key)\n","      batches = jax.random.permutation(shuffle_key, t)\n","      batches = batches.reshape(BATCH_SIZE, -1)\n","      for batch in batches:\n","          params, opt_state, loss_value = step(params, opt_state, batch)\n","      if epoch % 100 == 0:\n","          print(f'epoch {epoch}, loss: {loss_value}')\n","\n","  return params\n","\n","train_key, key = jax.random.split(key)\n","params = fit(initial_params, OPTIMIZER, key)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def sol(t):\n","    r1 = - c / (2 * m) + 1 / (2*m) * (c ** 2 - 4 * k * m) ** (1 / 2)\n","    r2 = - c / (2 * m) - 1 / (2*m) * (c ** 2 - 4 * k * m) ** (1 / 2)\n","    c2 = r1 / (r1 - r2)\n","    c1 = 1 - c2\n","    return c1 * jnp.exp(r1 * t) + c2 * jnp.exp(r2 * t)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","model = jax.vmap(lambda t: net(params, t))\n","\n","plt.plot(t, jnp.real(sol(t)), label='true_sol')\n","plt.plot(t, model(t), label='pred_sol')\n","plt.legend()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
