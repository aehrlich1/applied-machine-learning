{"cells":[{"cell_type":"code","execution_count":70,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-20T19:43:24.439682Z","iopub.status.busy":"2022-11-20T19:43:24.439162Z","iopub.status.idle":"2022-11-20T19:43:26.929679Z","shell.execute_reply":"2022-11-20T19:43:26.928507Z","shell.execute_reply.started":"2022-11-20T19:43:24.439591Z"},"trusted":true},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import jit, random\n","import numpy as np\n","import optax\n","\n","BATCH_SIZE = 10\n","LEARNING_RATE = 1e-2\n","LAYER_SIZES = [1, 30, 50, 30, 1]\n","NUM_EPOCHS = 5_000\n","#OPTIMIZER = optax.adam(learning_rate=LEARNING_RATE)\n","OPTIMIZER = optax.yogi(learning_rate=LEARNING_RATE)\n","#OPTIMIZER = optax.adabelief(learning_rate=LEARNING_RATE)\n","#OPTIMZER = optax.fromage\n","#OPTIMIZER = optax.optimistic_gradient_descent(learning_rate=LEARNING_RATE)\n","ACTIVATION = jnp.tanh\n","#ACTIVATION = jax.nn.relu\n","#ACTIVATION = jax.nn.sigmoid\n","#ACTIVATION = jax.nn.silu\n","#ACTIVATION = jax.nn.softplus\n","#ACTIVATION = jax.nn.softmax\n","\n","t = jnp.linspace(start=0, stop=20, num=1000)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- See [Activation Functions](https://jax.readthedocs.io/en/latest/jax.nn.html?highlight=sigmoid) for a list of possible activations.\n","- See [Optimizers](https://optax.readthedocs.io/en/latest/api.html#optax.yogi) for a list of optimizers."]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["# A helper function to randomly initialize weights and biases\n","# for a dense neural network layer\n","def random_layer_params(m, n, key, scale=1e-2):\n","  w_key, b_key = random.split(key)\n","  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,1))\n","\n","# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n","def init_network_params(sizes, key):\n","  \"\"\"\n","  returns:\n","  <list> of length: (#layers - 1)\n","\n","  Each list item contains a <tuple> of len(): 2 (Weights and Biases)\n","\n","  Each tuple consists of 2 Arrays of dimension: \n","\n","  \"\"\"\n","  \n","  keys = random.split(key, len(sizes))\n","  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n","\n","initial_params = init_network_params(LAYER_SIZES, random.PRNGKey(0))"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2022-11-20T19:43:39.140148Z","iopub.status.busy":"2022-11-20T19:43:39.139751Z","iopub.status.idle":"2022-11-20T19:43:39.302888Z","shell.execute_reply":"2022-11-20T19:43:39.301707Z","shell.execute_reply.started":"2022-11-20T19:43:39.140116Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Array(0.02171806, dtype=float32)"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["def net(params, x):\n","    activations = x\n","    for w, b in params[:-1]:\n","        outputs = jnp.dot(w, activations) + b\n","        activations = ACTIVATION(outputs)\n","\n","    final_w, final_b = params[-1]\n","    output = jnp.dot(final_w, activations) + final_b\n","    return output.reshape()\n","\n","# check\n","net(initial_params, 2.0)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"data":{"text/plain":["Array(0.02171571, dtype=float32)"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["m = 1.\n","c = 0.1\n","k = 1.\n","y0 = 1.\n","y0_prime = 0.\n","\n","@jit\n","def ode(params, t):\n","    nn = lambda t: net(params, t)\n","    dnn = jax.grad(nn)\n","    ddnn = jax.grad(dnn)\n","    return m * ddnn(t) + c * dnn(t) + k * nn(t)\n","\n","# check\n","ode(initial_params, 3.0)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"data":{"text/plain":["Array(25.000963, dtype=float32)"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["ode_batched = jax.vmap(ode, in_axes=[None, 0])\n","dnn = jit(jax.grad(net, argnums=1))\n","\n","@jit\n","def loss(params: optax.Params, batch: jnp.ndarray) -> jnp.ndarray:\n","    y_hat = ode_batched(params, batch)\n","    ode_loss = jnp.mean(y_hat ** 2)\n","    init_loss_1 = (net(params, 0.) - y0) ** 2\n","    init_loss_2 = (dnn(params, 0.) - y0_prime) ** 2\n","\n","    return ode_loss + init_loss_1 + init_loss_2\n","\n","# check\n","loss(initial_params, jnp.array([1., 2.]))"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0, loss: 24.99991798400879\n","epoch 100, loss: 0.22577229142189026\n","epoch 200, loss: 0.18385349214076996\n","epoch 300, loss: 0.11211229860782623\n","epoch 400, loss: 0.15990521013736725\n","epoch 500, loss: 0.14494380354881287\n","epoch 600, loss: 0.09249947965145111\n","epoch 700, loss: 0.08992071449756622\n","epoch 800, loss: 0.10801669210195541\n","epoch 900, loss: 0.09417270123958588\n","epoch 1000, loss: 0.10328584909439087\n","epoch 1100, loss: 0.09043273329734802\n","epoch 1200, loss: 0.0931827649474144\n","epoch 1300, loss: 0.10687728971242905\n","epoch 1400, loss: 0.08369840681552887\n","epoch 1500, loss: 0.08637767285108566\n","epoch 1600, loss: 0.08680921047925949\n","epoch 1700, loss: 0.09926927834749222\n","epoch 1800, loss: 0.09174534678459167\n","epoch 1900, loss: 0.10683055222034454\n","epoch 2000, loss: 0.07871776074171066\n","epoch 2100, loss: 0.09929276257753372\n","epoch 2200, loss: 0.07580594718456268\n","epoch 2300, loss: 0.06466004252433777\n","epoch 2400, loss: 0.10614433139562607\n","epoch 2500, loss: 0.08196689933538437\n","epoch 2600, loss: 0.06500308960676193\n","epoch 2700, loss: 0.06834430992603302\n","epoch 2800, loss: 0.07085523009300232\n","epoch 2900, loss: 0.0471043735742569\n","epoch 3000, loss: 0.0719604641199112\n","epoch 3100, loss: 0.06628169864416122\n","epoch 3200, loss: 0.06251868605613708\n","epoch 3300, loss: 0.06844457983970642\n","epoch 3400, loss: 0.05326838046312332\n","epoch 3500, loss: 0.041719913482666016\n","epoch 3600, loss: 0.03691850230097771\n","epoch 3700, loss: 0.040069274604320526\n","epoch 3800, loss: 0.03640861064195633\n","epoch 3900, loss: 0.03342597559094429\n","epoch 4000, loss: 0.02834387868642807\n","epoch 4100, loss: 0.02714352309703827\n","epoch 4200, loss: 0.028620848432183266\n","epoch 4300, loss: 0.024471472948789597\n","epoch 4400, loss: 0.02071385458111763\n","epoch 4500, loss: 0.014096195809543133\n","epoch 4600, loss: 0.015618590638041496\n","epoch 4700, loss: 0.013260581530630589\n","epoch 4800, loss: 0.00939745083451271\n","epoch 4900, loss: 0.013494566082954407\n"]}],"source":["key = jax.random.PRNGKey(42)\n","\n","def fit(params: optax.Params, optimizer: optax.GradientTransformation, key) -> optax.Params:\n","  opt_state = optimizer.init(params)\n","\n","  @jit\n","  def step(params, opt_state, batch):\n","    loss_value, grads = jax.value_and_grad(loss)(params, batch)\n","    updates, opt_state = optimizer.update(grads, opt_state, params)\n","    params = optax.apply_updates(params, updates)\n","    return params, opt_state, loss_value\n","\n","  for epoch in range(NUM_EPOCHS):\n","      shuffle_key, key = jax.random.split(key)\n","      batches = jax.random.permutation(shuffle_key, t)\n","      batches = batches.reshape(BATCH_SIZE, -1)\n","      for batch in batches:\n","          params, opt_state, loss_value = step(params, opt_state, batch)\n","      if epoch % 100 == 0:\n","          print(f'epoch {epoch}, loss: {loss_value}')\n","\n","  return params\n","\n","train_key, key = jax.random.split(key)\n","params = fit(initial_params, OPTIMIZER, key)\n"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["def sol(t):\n","    r1 = - c / (2 * m) + 1 / (2*m) * (c ** 2 - 4 * k * m) ** (1 / 2)\n","    r2 = - c / (2 * m) - 1 / (2*m) * (c ** 2 - 4 * k * m) ** (1 / 2)\n","    c2 = r1 / (r1 - r2)\n","    c1 = 1 - c2\n","    return c1 * jnp.exp(r1 * t) + c2 * jnp.exp(r2 * t)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","model = jax.vmap(lambda t: net(params, t))\n","\n","plt.plot(t, jnp.real(sol(t)), label='true_sol')\n","plt.plot(t, model(t), label='pred_sol')\n","plt.legend()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
